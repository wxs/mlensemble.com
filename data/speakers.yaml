- name: "Yevgeniy Valis"
  position: "Director of Applied ML"
  org: "Borealis AI"
  image: "/headshots/yevgeniy.jpg"
  bio: >
     Yevgeniy Vahlis is the head of the applied machine learning group at
     Borealis AI. Prior to joining Borealis AI Yevgeniy lead an applied
     research team at Georgian Partners, a late stage venture capital fund, and
     worked at a number of tech companies including Amazon and Nymi. Yevgeniy
     started his career at AT&T Labs in New York as a research scientist after
     completing his PhD in computer science at UofT and spending a year at
     Columbia university as a postdoc."
  title: ""
  abstract: ""
- name: "Martin Snelgrove"
  org: "Untether AI"
  image: "/headshots/snelgrove.jpg"
  title: "Tensor Stasis: Inference without kilowatts"
  bio: >
      Martin is a partly reformed academic: degrees and ECE Prof. at U. of Toronto,
      sabbatical time at AT&T Bell Labs and then an Industrial Research Chair at
      Carleton. He followed his lab when it started a chip shop called Philsar, then
      was CTO for a company that did early LTE-type wireless internet, then a stack
      of consulting. For the last ten years he’s been CEO of Kapik, which designs
      strange mixed-signal ICs in Toronto and thereabouts.

      Back in the 90s he was involved in early massively-parallel computing work in
      “smart memory”, which was the most power-efficient but a highly
      specialized way to do video: and now it turns out that dot products are
      very good; burning energy is very bad; and transistors are very small; so
      the technology is back. He’s CEO of “Untether”, doing a chip with seed
      round funding and getting ready for A round and turmoil.
     
  abstract: >
    The energy consumed in doing heavy ML inference is a problem: it kills
    battery life and heats up data centres. Cloud computing fixes nothing,
    because the energy your device uses to ship the data is worse than what it
    needs to do the work itself.

    I’ll cover a bit of the physics of where energy goes; but it turns out that
    almost all of it is wasted pumping data around. The trendy ways to do that
    are “in-memory” and “near-memory” computing, one of which works at scale.

    From a software point of view, programming near memory is a serious
    mind-warp. A GPU runs around 1,000 processors in parallel; we run a
    million. On one chip. On a AAA cell.
    
- name: "Julieta Martinez"
  org: "Uber ATG"
  image: "/headshots/julieta.jpg"
  title: "Info to come"
  bio: >
      Julieta is a computer vision researcher focused on machine learning, deep
      learning, and large-scale retrieval. She is currently a researcher with
      Uber ATG Toronto. She enjoys coming up with simple, scalable, and
      easy-to-understand algorithms that address challenging research problems.
      Her work has been published in the top three computer vision venues
      (CVPR/ECCV/ICCV). Julieta obtained her PhD from UBC and has interned at
      Disney Research and the Max Planck Institute. 
  abstract: ""

- name: "Oren Kraus"
  org: "Phenomic.ai"
  image: "/headshots/oren.jpg"
  title: "Classifying and segmenting microscopy images with deep multiple instance learning"
  bio: >
    I'm the co-founder of Phenomic AI; a startup accelerating image-based drug
    discovery with AI. Previously, I completed my PhD in Brendan Frey's lab. My
    research focused on applying cutting edge machine learning techniques
    (specifically deep learning) to high throughput microscopy screens of cell
    biology. In collaboration with Charlie Boone and Brenda Andrews at the
    Donnelly Centre for Cellular and Biomolecular Research (CCBR), I generated
    datasets and trained deep learning models on millions of individual cell
    objects from genome wide microscopy screens. Previously, I completed my
    BASc and MASc in mechanical and biomedical engineering at the University of
    Toronto. I also interned as a machine learning researcher at Apple in
    California and Borealis AI in Toronto.
  abstract: >
    High-content screening (HCS) technologies have enabled large scale imaging
    experiments for studying cell biology and for drug screening. These systems
    produce hundreds of thousands of microscopy images per day and their
    utility depends on automated image analysis. Recently, deep learning
    approaches that learn feature representations directly from pixel intensity
    values have dominated object recognition challenges. These tasks typically
    have a single centered object per image and existing models are not
    directly applicable to microscopy datasets. Here we develop an approach
    that combines deep convolutional neural networks (CNNs) with multiple
    instance learning (MIL) in order to classify and segment microscopy images
    using only whole image level annotations. We introduce a new neural network
    architecture that uses MIL to simultaneously classify and segment
    microscopy images with populations of cells. We base our approach on the
    similarity between the aggregation function used in MIL and pooling layers
    used in CNNs. To facilitate aggregating across large numbers of instances
    in CNN feature maps we present the Noisy-AND pooling function, a new MIL
    operator that is robust to outliers. Combining CNNs with MIL enables
    training CNNs using whole microscopy images with image level labels. We
    show that training end-to-end MIL CNNs outperforms several previous methods
    on both mammalian and yeast datasets without requiring any segmentation
    steps.
      
    #- name: "Jennifer Lisgtarten"
    #org: "UC Berkeley"
    #image: "/headshots/listgarten.jpg"
    #title: "Info to come"
    #bio: ""
    #abstract: ""

- name: "Julia Powles"
  org: "New York University"
  image: "/headshots/powlesb.jpg"
  title: "Info to come"
  bio: >
    Julia Powles is a legal researcher at the University of Cambridge, on two
    pioneering tech-law projects, on a cross-appointment between the Faculty of
    Law and the Computer Laboratory.

    Dr Powles has nearly a decade of experience in intellectual property law.
    Her PhD at the University of Cambridge concerned the inventive concept in
    patent law. This built on her broader expertise in intellectual property
    developed in private practice at Minter Ellison Lawyers, during her
    master’s degree at the University of Oxford, and as speechwriter for the
    Director General of the World Intellectual Property Organization in Geneva.

    Dr Powles holds undergraduate honours degrees in law from the University of
    Western Australia and science from the Australian National University. She
    has worked on scientific research projects in medical genetics,
    environmental risk, biophysics and plant physiology, and clerked for
    Justice Garry Downes AM when he was a judge of the Federal Court of
    Australia and President of the Commonwealth Administrative Appeals
    Tribunal. The latter position involved work on administrative law,
    intellectual property and national security.
      
  abstract: ""
